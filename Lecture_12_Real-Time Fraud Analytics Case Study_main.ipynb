{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML System Design Class Notes: Real-Time Fraud Analytics Case Study\n",
    "\n",
    "## Overview\n",
    "In this class, we explored the design and implementation of real-time fraud detection systems in the financial technology (fintech) domain, particularly focused on transaction processing. The class discussed various statistical tests, performance metrics, AWS services, and the architecture necessary for building an efficient and scalable fraud detection system.\n",
    "\n",
    "---\n",
    "\n",
    "## What are the metrics used in Fraud Analytics in Transactions?\n",
    "\n",
    "Fraud analytics employs several metrics to evaluate model performance. Understanding these metrics is crucial for assessing the effectiveness of fraud detection models.\n",
    "\n",
    "### 1. Precision\n",
    "Precision evaluates the accuracy of positive predictions:\n",
    "$$ \n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}} \n",
    "$$\n",
    "- **Interpretation**: High precision means that when the system predicts fraud, it is likely correct. This is crucial in contexts where false positives can lead to significant financial and reputational losses.\n",
    "\n",
    "### 2. Recall\n",
    "Recall, also known as sensitivity, assesses the completeness of the positive predictions:\n",
    "$$ \n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}} \n",
    "$$\n",
    "- **Interpretation**: High recall means that the system successfully identifies a majority of actual fraudulent transactions, vital when failing to detect fraud incurs heavy costs.\n",
    "\n",
    "### 3. F1 Score\n",
    "The F1 Score provides a balance between precision and recall:\n",
    "$$ \n",
    "\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \n",
    "$$\n",
    "- **Interpretation**: It offers a single score to evaluate the model's overall accuracy, especially useful with uneven class distributions.\n",
    "\n",
    "### 4. AUC-ROC\n",
    "AUC (Area Under Curve) of the ROC (Receiver Operating Characteristic) curve evaluates how well the model distinguishes between classes across different threshold settings.\n",
    "\n",
    "### 5. Confusion Matrix\n",
    "A confusion matrix is a table that visualizes the performance of an algorithm by showing True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) values. This matrix provides insights into where the model is making errors.\n",
    "\n",
    "---\n",
    "\n",
    "## Difference between Precision, Recall, and F1 Score\n",
    "\n",
    "Choosing between precision, recall, and the F1 score depends on the context:\n",
    "\n",
    "- Use **Precision** when the cost of a false positive is high (e.g., blocking legitimate transactions).\n",
    "- Use **Recall** when the cost of a false negative is high (i.e., missing a fraudulent transaction).\n",
    "- Use **F1 Score** when it is crucial to maintain a balance between precision and recall, especially with uneven class distribution.\n",
    "\n",
    "### Summary of Trade-offs\n",
    "Considerations for metrics in fraud detection:\n",
    "- High precision minimizes customer dissatisfaction from false blocks.\n",
    "- High recall enhances overall security by catching as many fraudulent transactions as possible.\n",
    "- The F1 score is crucial in scenarios where you cannot compromise on either of the two.\n",
    "\n",
    "---\n",
    "\n",
    "## What are AWS Services Used for End-to-End Real-Time Fraud Analytics?\n",
    "\n",
    "The architecture of a real-time fraud detection system in AWS comprises various services:\n",
    "\n",
    "### 1. Amazon Kinesis Data Streams\n",
    "Used for ingesting high-throughput transactions from diverse sources, capable of handling up to **10,000 transactions per second**.\n",
    "\n",
    "### 2. Amazon Kinesis Data Analytics (Apache Flink)\n",
    "Utilized for real-time data analytics and processing â€” allowing for the immediate analysis of incoming transaction data.\n",
    "\n",
    "### 3. DynamoDB\n",
    "Provides low-latency lookups, enabling swift access to metadata related to transactions.\n",
    "\n",
    "### 4. Amazon SageMaker\n",
    "Facilitates model training, hosting, and monitoring of machine learning models.\n",
    "\n",
    "### 5. AWS Lambda\n",
    "Allows for executing code in response to transactions without the need for provisioning servers.\n",
    "\n",
    "### 6. Amazon API Gateway\n",
    "To create and manage APIs that facilitate communication between the fraud detection service and other applications.\n",
    "\n",
    "### 7. Amazon SNS (Simple Notification Service)\n",
    "Used for sending SMS or email alerts to customers or customer service regarding flagged transactions.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Amazon Kinesis Data Streams]\n",
    "    B[Amazon Kinesis Data Analytics]\n",
    "    C[DynamoDB]\n",
    "    D[Amazon SageMaker]\n",
    "    E[AWS Lambda]\n",
    "    F[Amazon API Gateway]\n",
    "    G[Amazon SNS]\n",
    "\n",
    "    A -->|Transaction Ingestion| B\n",
    "    B --> C\n",
    "    B --> D\n",
    "    D --> E\n",
    "    E --> F\n",
    "    E --> G\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10,000 Transactions per Second - High Throughput\n",
    "\n",
    "To achieve high throughput for processing transactions using **Amazon Kinesis Data Streams**, sharding is essential:\n",
    "\n",
    "### Sharding\n",
    "- Divide Kinesis data streams into shards.\n",
    "- Each shard has a capacity of **1 MB/sec** input or **1,000 records per second**.\n",
    "  \n",
    "#### Configuration Example\n",
    "For managing **10,000 transactions per second**, configure **10 shards**. This architecture is critical for maintaining throughput during peak transaction times.\n",
    "\n",
    "---\n",
    "\n",
    "## What Happens in a Lifestyle Store POS Machine?\n",
    "\n",
    "When a transaction occurs in a Point of Sale (POS) system:\n",
    "\n",
    "1. The POS machine captures transaction data (e.g., transaction ID, user ID, amount, timestamp, merchant ID).\n",
    "2. This data is sent to the bank's transaction processing system, which aggregates and analyzes it for fraud detection.\n",
    "\n",
    "### Who Will Get Maximum Share of 2.5% of Sales?\n",
    "The **Issuer Bank (e.g., ICICI)** usually receives the largest portion of the transaction fee due to the risks associated with credit issuance.\n",
    "\n",
    "### Why Does the Issuer Bank Receive Maximum Share?\n",
    "The issuer bank is primarily responsible for:\n",
    "- Credit approval\n",
    "- Fraud investigation\n",
    "- Handling chargebacks\n",
    "Thus, it incurs more risk compared to other parties involved in the transaction.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-Time Feature Engineering\n",
    "\n",
    "The **Apache Flink** backend can perform real-time feature engineering tasks such as:\n",
    "\n",
    "- Generating rolling aggregates (e.g., cumulative spending over the last hour).\n",
    "- Enriching transaction data by cross-referencing with user or merchant data stored in **DynamoDB**.\n",
    "\n",
    "### Example of a Real-Time Feature\n",
    "- **Rolling Spend**: Amount spent in the last 1 hour that can enhance the fraud detection model greatly.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Dwell Time?\n",
    "**Dwell time** refers to the amount of time a customer spends in a store before they leave. Leveraging this metric can help analyze purchasing patterns and signal potential fraudulent transactions.\n",
    "\n",
    "---\n",
    "\n",
    "## Where to Store the Data?\n",
    "\n",
    "For persistent storage of transaction data and model artifacts, **Amazon S3** is the recommended solution. It serves as a staging area for data analysis and model training, allowing for durability and availability.\n",
    "\n",
    "---\n",
    "\n",
    "## Which AWS Service Used for Training, Hosting, and Monitoring Model?\n",
    "**Amazon SageMaker** is specifically designed for the entire machine learning lifecycle, from data preparation to model deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## Which Service for Batch Feature Engineering for Historical Data?\n",
    "\n",
    "**Amazon EMR** (Elastic MapReduce) is utilized for batch processing and large-scale feature engineering of historical data.\n",
    "\n",
    "---\n",
    "\n",
    "## Explain Anomaly Detection Methods\n",
    "\n",
    "### Isolation Forest\n",
    "Isolation Forest is an anomaly detection algorithm that partitions data into isolated sections. It identifies anomalies based on the number of partitions needed to isolate data points.\n",
    "\n",
    "### Autoencoder\n",
    "An autoencoder is a type of neural network designed to learn efficient representations of data. It comprises two main components:\n",
    "- **Encoder**: Compresses data into a lower-dimensional representation.\n",
    "- **Decoder**: Reconstructs the original data from the encodings.\n",
    "\n",
    "Anomalies are detected through reconstruction errors (i.e., how far the predicted values are from the original values).\n",
    "\n",
    "---\n",
    "\n",
    "## Supervised vs Unsupervised Anomaly Detection\n",
    "\n",
    "### Unsupervised Anomaly Detection\n",
    "Algorithms like Isolation Forest and Autoencoders do not require labeled data. They learn normal patterns and recognize anomalies based on deviations from these patterns.\n",
    "\n",
    "### Supervised Anomaly Detection\n",
    "Involves training models using labeled data (fraud vs. non-fraud). It can include methods like logistic regression and decision trees, which rely on previously labeled datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## What is the Intent to Have Two End Points, One from XGBoost and One from Autoencoders?\n",
    "\n",
    "Having both models allows for:\n",
    "\n",
    "- **Comparative Analysis**: Testing and comparing performance to yield more reliable outcomes.\n",
    "- **Hybrid Approach**: Leveraging the strengths of both supervised (XGBoost) and unsupervised (Autoencoders) methods to improve overall detection accuracy.\n",
    "\n",
    "### What is the Loss Function?\n",
    "The loss function used in autoencoders is typically the **Mean Squared Error (MSE)**, which quantifies the average of the squares of the errors, representing how far predicted values are from actual values.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Location Velocity?\n",
    "**Location Velocity** gauges the speed at which a customer moves between transactions based on geographic locations, serving as a useful feature for identifying anomalous behaviors.\n",
    "\n",
    "## What is Geo-Velocity?\n",
    "**Geo-Velocity** indicates average speed calculations that assist in analyzing transaction legitimacy based on the time and distance traveled between significant transactions. This metric helps highlight instances of impossible travel, which could indicate fraud.\n",
    "\n",
    "---\n",
    "\n",
    "## Features for Blocking Transactions\n",
    "\n",
    "Identification of high-risk transactions may rely on:\n",
    "1. **Fraud Score > 0.9**\n",
    "2. **Transaction Amount > $1,000**\n",
    "3. **Geo-Velocity Analysis for Impossible Travel**\n",
    "\n",
    "---\n",
    "\n",
    "## How Blocking Happens\n",
    "\n",
    "1. **AWS API Gateway**: Facilitates requests to a backend processing system to block fraudulent transactions.\n",
    "2. **AWS Lambda**: Automatically triggers responses based on predefined conditions to review transactions and decide on blocking.\n",
    "3. **Amazon SNS**: Sends notifications and alerts via SMS or email to respective stakeholders about flagged transactions.\n",
    "\n",
    "---\n",
    "\n",
    "## Monitoring and Retraining Phase\n",
    "\n",
    "### Data Drift Detection\n",
    "To stay effective over time, it's necessary to monitor for data drift. Statistical tests such as **KL Divergence**, **Chi-Squared Test**, or **KS Test** can evaluate shifts in data distribution over time.\n",
    "\n",
    "### Experimental Model A vs. Model B\n",
    "When comparing performance between two models, statistical tests can be utilized:\n",
    "- **Z-Test** for proportions if comparing the success rates of two proportions.\n",
    "- **T-Test** to compare means between two groups, particularly with small sample sizes.\n",
    "\n",
    "---\n",
    "\n",
    "### Proportion Example\n",
    "If the historical fraud rate is 0.1% and today, out of 10,000 transactions, 120 were observed as fraudulent, you'd compare this against an expected count of 100. This comparison (observed - expected = 120 - 100 = 20) can be statistically tested using a Z-Test or T-Test to confirm if the observed increase is significant.\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Efficiency - CPU vs. GPU\n",
    "\n",
    "When considering resource allocation for model training:\n",
    "- **GPU instances**: Ideal for deep learning tasks that require significant computational power.\n",
    "- **CPU instances**: Often more cost-effective for traditional machine learning algorithms like XGBoost.\n",
    "\n",
    "### Key Considerations\n",
    "- **Training Complexity**: More complex models may necessitate the use of GPUs.\n",
    "- **Budget Constraints**: CPUs may be more suitable for simpler models with lower training costs.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Combined Fraud Score?\n",
    "\n",
    "The combined fraud score merges the outputs from both XGBoost (providing predictive probability) and autoencoders (providing reconstruction error). By weighting these scores, practitioners can achieve a unified output that enhances overall detection accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "For comprehensive insights on implementing statistical hypothesis tests in Python, refer to the [Statistical Hypothesis Tests in Python Cheat Sheet](https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/).\n",
    "\n",
    "---\n",
    "\n",
    "This class provided a comprehensive overview of the architecture and considerations necessary for effective real-time fraud detection in transactions. For further questions or clarification, please feel free to ask! \n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[POS Machine] -->|Transaction Data| B[Kinesis Data Streams]\n",
    "    B --> C[Kinesis Data Analytics\\n Apache Flink]\n",
    "    C -->|Real-Time Features| D[DynamoDB\\n Metadata Lookup]\n",
    "    C --> E[SageMaker\\n Fraud Prediction]\n",
    "    E -->|Fraud Score| F[Lambda\\n Decision Engine]\n",
    "    F -->|Block/Allow| G[API Gateway]\n",
    "    F -->|Alert| H[SNS]\n",
    "    G --> I[Bank/Customer Systems]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Metrics in Fraud Analytics\n",
    "\n",
    "### 1. Precision vs. Recall Trade-off\n",
    "Understanding when to prioritize precision or recall is fundamental to developing effective fraud detection systems.\n",
    "\n",
    "```mermaid\n",
    "pie\n",
    "    title Metric Selection Criteria\n",
    "    \"Precision  Minimize False Positives\" : 40\n",
    "    \"Recall  Minimize False Negatives\" : 60\n",
    "```\n",
    "- Precision is critical when blocking legitimate transactions is costly, while recall is essential when missing fraud is unacceptable.\n",
    "\n",
    "### 2. Confusion Matrix\n",
    "Visualization of true and false predictions helps in understanding model performance.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Actual Fraud] -->|True Positive| B[Predicted Fraud]\n",
    "    A -->|False Negative| C[Predicted Legit]\n",
    "    D[Actual Legit] -->|False Positive| B\n",
    "    D -->|True Negative| C\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## AWS Architecture for 10K TPS\n",
    "\n",
    "### High-Throughput Design\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Kinesis\n",
    "        B[Shard 1\\n1K TPS]\n",
    "        C[Shard 2\\n1K TPS]\n",
    "        D[...]\n",
    "        E[Shard 10\\n1K TPS]\n",
    "    end\n",
    "    A[POS Machines] --> Kinesis\n",
    "    Kinesis --> F[Flink Processing]\n",
    "```\n",
    "\n",
    "- **Sharding**: 10 shards handle 10K transactions/sec with 1K TPS per shard.\n",
    "\n",
    "---\n",
    "\n",
    "## Fraud Detection Techniques\n",
    "\n",
    "### Anomaly Detection Methods\n",
    "Utilizing various methods for detecting anomalies enhances fraud prevention capabilities.\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Anomaly Detection] --> B[Supervised\\nXGBoost]\n",
    "    A --> C[Unsupervised\\nAutoencoder]\n",
    "    C --> D[Reconstruction Error]\n",
    "    B --> E[Probability Score]\n",
    "    D & E --> F[Combined Fraud Score]\n",
    "```\n",
    "\n",
    "### Hybrid Approach Benefits\n",
    "- **XGBoost** utilizes labeled historical data for prediction.\n",
    "- **Autoencoder** captures novel patterns of fraud through unsupervised learning.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "### Real-Time Features\n",
    "| Feature              | Calculation                           | Example Value         |\n",
    "|---------------------|---------------------------------------|-----------------------|\n",
    "| Location Velocity    | Distance/ Time between transactions    | 500 km/h - Alert Fraud! |\n",
    "| Rolling Spend        | Cumulative sum of amount in last 1 hour | $2,000                |\n",
    "\n",
    "---\n",
    "\n",
    "## Transaction Blocking Logic\n",
    "\n",
    "Decision flow for blocking transactions based on evaluation criteria.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Fraud Score > 0.9?] -->|Yes| B[Block]\n",
    "    A -->|No| C[Amount > $1K?] -->|Yes| D[Manual Review]\n",
    "    C -->|No| E[Allow]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Monitoring & Retraining\n",
    "\n",
    "### Data Drift Detection\n",
    "Monitoring for shifts in data distribution to ensure model effectiveness.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[New Transactions] --> B[KS Test vs. Training Data]\n",
    "    B -->|Drift Detected| C[Retrain Model]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Concepts of MLOps, LLMOps, and DevOps\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "    root((MLOps vs. LLMOps vs. DevOps))\n",
    "        MLOps\n",
    "            Focus: Machine Learning Models\n",
    "            Key Tools: SageMaker, MLflow\n",
    "            Challenges: Data drift, model retraining\n",
    "        LLMOps\n",
    "            Focus: Large Language Models\n",
    "            Key Tools: LangChain, LLamaIndex\n",
    "            Challenges: Prompt engineering, hallucination control\n",
    "        DevOps\n",
    "            Focus: Software Deployment\n",
    "            Key Tools: Jenkins, Kubernetes\n",
    "            Challenges: CI/CD, infrastructure scaling\n",
    "```\n",
    "\n",
    "### Detailed Comparison\n",
    "| Aspect             | MLOps                  | LLMOps                 | DevOps               |\n",
    "|--------------------|------------------------|------------------------|----------------------|\n",
    "| **Primary Goal**    | Deploy ML models       | Deploy LLM pipelines    | Deploy software      |\n",
    "| **Key Tools**      | SageMaker, Kubeflow    | LangChain, Weaviate    | Docker, Kubernetes   |\n",
    "| **Unique Challenges**| Data versioning       | Prompt management       | Infrastructure as Code|\n",
    "| **Testing**        | Model accuracy tests    | Prompt effectiveness    | Unit/integration tests|\n",
    "\n",
    "### Summary\n",
    "- **MLOps**: Optimized for traditional machine learning scenarios such as fraud detection models.\n",
    "- **LLMOps**: Addresses LLM-specific requirements, including RAG (Retrieve and Generate) pipelines.\n",
    "- **DevOps**: Aiming for efficient software deployment processes across platforms.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
