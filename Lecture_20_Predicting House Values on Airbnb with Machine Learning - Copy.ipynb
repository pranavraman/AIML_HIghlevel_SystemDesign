{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0347a924",
   "metadata": {},
   "source": [
    "### Predicting House Values on Airbnb with Machine Learning\n",
    "\n",
    "**Key Points:**\n",
    "- Machine learning can likely predict Airbnb house values more accurately than traditional methods by analyzing complex data patterns.\n",
    "- Key datasets include listings, host data, user behavior, and market trends, which drive accurate predictions.\n",
    "- Lifetime Value (LTV) and Customer Acquisition Cost (CAC) metrics guide pricing and marketing strategies.\n",
    "- System design involves data ingestion, preprocessing, modeling, and real-time serving with tools like Spark and FastAPI.\n",
    "- Non-functional requirements like low latency and GDPR compliance are critical for production systems.\n",
    "\n",
    "**Overview**  \n",
    "Building a machine learning system to predict house values (or rent) on Airbnb involves using data-driven models to estimate listing prices. This approach seems to outperform traditional formula-based methods by leveraging diverse data sources, such as listing details and user behavior, to capture complex patterns. The system aims to optimize pricing, enhance listing visibility, and inform marketing strategies.\n",
    "\n",
    "**Why Machine Learning?**  \n",
    "Traditional methods, like fixed pricing formulas, may oversimplify factors like location or amenities. Machine learning models, such as XGBoost, can analyze historical data to predict values with higher accuracy, as evidenced by lower error metrics like RMSE.\n",
    "\n",
    "**Business Impact**  \n",
    "Predicting house values helps Airbnb optimize host pricing and guest acquisition. By estimating LTV, the platform can balance acquisition costs (CAC) to ensure profitability, targeting a sustainable LTV-to-CAC ratio (ideally 3:1).\n",
    "\n",
    "**System Components**  \n",
    "The system includes data collection, feature engineering (e.g., calculating location scores), preprocessing, modeling, and deployment. Technologies like Apache Spark for data processing and FastAPI for real-time APIs ensure scalability and low latency.\n",
    "\n",
    "**Considerations**  \n",
    "The system must meet functional requirements (e.g., real-time predictions) and non-functional ones (e.g., <100 ms latency, GDPR compliance). Tools like SHAP provide explainability, while monitoring ensures model reliability.\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Notes for ML System Design Class: Predicting House Values on Airbnb\n",
    "\n",
    "These notes provide a comprehensive guide to designing a machine learning system for predicting house values on Airbnb, tailored for freshers but maintaining technical depth. The content is structured for clarity, includes definitions, formulas, code snippets, flowcharts, and practical advice, and is based on the lecture transcription provided.\n",
    "\n",
    "#### 1. Introduction to Predicting House Values on Airbnb\n",
    "\n",
    "**Objective**  \n",
    "The goal is to predict the rental value of Airbnb listings using machine learning. This enables:\n",
    "- **Optimal Pricing**: Setting competitive prices to maximize bookings.\n",
    "- **Market Insights**: Understanding trends to improve listing strategies.\n",
    "- **Business Decisions**: Guiding marketing and host support based on predicted value.\n",
    "\n",
    "**Why Machine Learning?**  \n",
    "Traditional pricing methods rely on static rules or simple formulas, which may not account for dynamic factors like seasonal demand or host reputation. Machine learning models learn from historical data, capturing complex relationships to provide more accurate predictions. For example, Airbnbâ€™s blog on [Customer Lifetime Value Prediction](https://medium.com/airbnb-engineering/predicting-customer-lifetime-value-at-airbnb-with-machine-learning-5a4756f66c6a) highlights how ML improves accuracy over rule-based systems.\n",
    "\n",
    "**Role of Lifetime Value (LTV)**  \n",
    "- **Definition**: LTV estimates the total revenue from a customer (host or guest) over their relationship with Airbnb.\n",
    "- **Formula**:  \n",
    "  \\[ \\text{LTV} = \\frac{\\text{Average Transaction Value} \\times \\text{Number of Transactions}}{\\text{Churn Rate}} \\]\n",
    "- **Use Case**: High LTV hosts may receive targeted incentives, as their listings generate more revenue. For instance, a host with frequent bookings and high ratings likely has a higher LTV, justifying marketing investment.\n",
    "\n",
    "#### 2. Metrics and Business Value\n",
    "\n",
    "**LTV and Customer Acquisition Cost (CAC)**  \n",
    "- **LTV**: Measures long-term revenue potential. For a host, this might be the total booking revenue over years.\n",
    "- **CAC**: The cost of acquiring a new customer, including marketing and onboarding expenses.\n",
    "  - Formula:  \n",
    "    \\[ \\text{CAC} = \\frac{\\text{Total Marketing and Sales Expenses}}{\\text{Number of New Customers Acquired}} \\]\n",
    "- **LTV-to-CAC Ratio**:  \n",
    "  \\[ \\text{LTV to CAC Ratio} = \\frac{\\text{LTV}}{\\text{CAC}} \\]  \n",
    "  A ratio of 3:1 or higher is typically sustainable, ensuring acquisition costs are justified.\n",
    "\n",
    "**Example Scenarios**  \n",
    "| Scenario | LTV () | CAC () | Ratio | Sustainable? |\n",
    "|----------|---------|---------|-------|--------------|\n",
    "| 1        | 300     | 100     | 3:1   | Yes          |\n",
    "| 2        | 200     | 100     | 2:1   | No           |\n",
    "| 3        | 400     | 150     | 2.67:1| Maybe        |\n",
    "\n",
    "**Business Impact**  \n",
    "Optimizing the LTV-to-CAC ratio helps Airbnb allocate resources efficiently, focusing on high-value hosts or guests while minimizing acquisition costs.\n",
    "\n",
    "#### 3. Data Collection and Feature Engineering\n",
    "\n",
    "**Key Datasets**  \n",
    "- **Listings Data**: Listing ID, price per night, bedrooms, bathrooms, amenities (e.g., Wi-Fi, pool).\n",
    "- **Host Data**: Response rate, superhost status, number of listings.\n",
    "- **User Behavior Data**: Bookings, reviews, search patterns.\n",
    "- **Market Data**: Location details, local events, economic trends.\n",
    "\n",
    "**Feature Engineering**  \n",
    "- **Historical Demand**: Number of bookings or inquiries, indicating listing popularity.\n",
    "- **Host Quality**: A score based on response rate and review ratings.\n",
    "- **Location Scores**: Distance to landmarks (e.g., subway stations) using the Haversine formula.\n",
    "\n",
    "**Haversine Formula**  \n",
    "Calculates the great-circle distance between two points on Earth:  \n",
    "\\[ \\text{haversin}(\\theta) = \\sin^2\\left(\\frac{\\theta}{2}\\right) \\]  \n",
    "\\[ a = \\text{haversin}(\\phi_2 - \\phi_1) + \\cos(\\phi_1) \\cdot \\cos(\\phi_2) \\cdot \\text{haversin}(\\lambda_2 - \\lambda_1) \\]  \n",
    "\\[ c = 2 \\cdot \\atan2\\left(\\sqrt{a}, \\sqrt{1-a}\\right) \\]  \n",
    "\\[ d = R \\cdot c \\]  \n",
    "Where \\( R = 6371 \\, \\text{km} \\), \\(\\phi\\) is latitude, and \\(\\lambda\\) is longitude.\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2 - lon1)\n",
    "    a = math.sin(delta_phi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "```\n",
    "\n",
    "#### 4. Data Preprocessing\n",
    "\n",
    "**Handling Missing Values**  \n",
    "- **Deletion**: Remove rows/columns with excessive missing data (use cautiously).\n",
    "- **Imputation**: Use mean/median for numerical data, mode for categorical, or advanced methods like KNN.\n",
    "\n",
    "**Encoding Categorical Variables**  \n",
    "- **One-Hot Encoding**: For low-cardinality features (e.g., city names).\n",
    "- **Label Encoding**: For ordinal features (e.g., rating levels).\n",
    "- **Embeddings**: For high-cardinality features (e.g., zip codes), capturing semantic relationships.\n",
    "\n",
    "**Exploratory Data Analysis (EDA)**  \n",
    "- **Histograms**: Show feature distributions.\n",
    "- **Box Plots**: Detect outliers.\n",
    "- **Correlation Matrix**: Identify feature relationships.\n",
    "\n",
    "**Example: Correlation Heatmap**  \n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig('correlation_matrix.png')\n",
    "```\n",
    "\n",
    "#### 5. Hypothesis Testing\n",
    "\n",
    "**Techniques**  \n",
    "- **T-tests**: Compare means of two groups (e.g., prices with/without 24-hour check-in).\n",
    "- **ANOVA**: Compare means across multiple groups (e.g., prices by neighborhood).\n",
    "- **Linear Regression**: Test relationships between variables.\n",
    "\n",
    "**Example Hypothesis**  \n",
    "- **H0**: No difference in booking rates with/without 24-hour check-in.  \n",
    "- **H1**: Significant difference exists.\n",
    "\n",
    "**T-test Example**  \n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "group1 = [25, 30, 28, 32, 29]  # Without 24-hour check-in\n",
    "group2 = [30, 35, 33, 37, 34]  # With 24-hour check-in\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject null hypothesis: Significant difference.\")\n",
    "else:\n",
    "    print(\"Fail to reject null hypothesis: No significant difference.\")\n",
    "```\n",
    "\n",
    "#### 6. Modeling\n",
    "\n",
    "**Model Options**  \n",
    "- **Linear Regression**: Simple, interpretable, but limited for non-linear data.\n",
    "- **XGBoost**: High performance, handles non-linear relationships (chosen for lower RMSE).\n",
    "- **Deep Neural Networks**: Suitable for complex patterns but resource-intensive.\n",
    "\n",
    "**Evaluation Metric**  \n",
    "- **RMSE**:  \n",
    "  \\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\]\n",
    "\n",
    "**XGBoost Example**  \n",
    "```python\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming X is features, y is target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "```\n",
    "\n",
    "#### 7. System Architecture\n",
    "\n",
    "**Pipeline Overview**  \n",
    "1. **Data Ingestion**: Use SQL or Spark for data collection.\n",
    "2. **Data Storage**: S3 for historical data, Redis for real-time.\n",
    "3. **Training Pipeline**: Orchestrate with Apache Airflow.\n",
    "4. **Model Serving**: Deploy via FastAPI/Flask APIs.\n",
    "5. **Monitoring**: Track performance and data drift.\n",
    "\n",
    "**Flowchart**  \n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Data Sources] --> B{Data Ingestion}\n",
    "    B --> C{Data Storage}\n",
    "    C --> D{Training Pipeline}\n",
    "    D --> E{Model Serving}\n",
    "    E --> F{Monitoring}\n",
    "    F --> G[Feedback Loop]\n",
    "```\n",
    "\n",
    "**Technologies**  \n",
    "- **SQL**: Relational data storage.\n",
    "- **Spark**: Big data processing.\n",
    "- **Redis**: Real-time data access.\n",
    "- **Airflow**: Workflow orchestration.\n",
    "- **FastAPI/Flask**: API development.\n",
    "\n",
    "#### 8. Functional and Non-Functional Requirements\n",
    "\n",
    "**Functional Requirements**  \n",
    "- Real-time predictions (<100 ms).\n",
    "- Explainability using SHAP values.\n",
    "\n",
    "**Non-Functional Requirements**  \n",
    "- **Latency**: <100 ms for real-time predictions.\n",
    "- **Scalability**: Handle large data volumes.\n",
    "- **Reliability**: 99.99% uptime.\n",
    "- **Maintainability**: Easy updates.\n",
    "- **Compliance**: GDPR for data privacy.\n",
    "\n",
    "#### 9. Explainability and Monitoring\n",
    "\n",
    "**Explainability with SHAP**  \n",
    "SHAP values assign contributions to each feature for a prediction, enhancing transparency. For example, SHAP can show that proximity to a landmark increases a listingâ€™s predicted value.\n",
    "\n",
    "**SHAP Example**  \n",
    "```python\n",
    "import shap\n",
    "\n",
    "# Assuming model is trained and X_test is available\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "```\n",
    "\n",
    "**Monitoring**  \n",
    "- **Drift Detection**: Monitor input data changes using tools like Evidently.\n",
    "- **Performance Tracking**: Continuously evaluate RMSE.\n",
    "- **Alerting**: Notify when performance degrades.\n",
    "\n",
    "#### 10. Practical Advice and Career Insights\n",
    "\n",
    "**Tech Blogs**  \n",
    "Studying blogs from companies like Airbnb ([Airbnb Engineering Blog](https://medium.com/airbnb-engineering)) or Zomato provides insights into real-world ML applications. For example, Airbnbâ€™s LTV prediction blog details practical challenges and solutions.\n",
    "\n",
    "**Interview Preparation**  \n",
    "- Learn tools: Airflow, Spark, FastAPI, SHAP.\n",
    "- Practice designing ML systems for problems like recommendation or pricing.\n",
    "- Understand case studies to discuss industry applications.\n",
    "\n",
    "**Summary**  \n",
    "This system design for predicting Airbnb house values involves collecting diverse data, engineering features like location scores, preprocessing, modeling with XGBoost, and deploying with scalable technologies. Explainability and monitoring ensure reliability, while studying industry blogs prepares freshers for careers in ML engineering.\n",
    "\n",
    "**Key Citations**  \n",
    "- [Airbnb Engineering: Predicting Customer Lifetime Value](https://medium.com/airbnb-engineering/predicting-customer-lifetime-value-at-airbnb-with-machine-learning-5a4756f66c6a)\n",
    "- [SHAP Documentation for Model Explainability](https://shap.readthedocs.io/en/latest/)\n",
    "- [XGBoost Documentation for Machine Learning](https://xgboost.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d87e2d2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
