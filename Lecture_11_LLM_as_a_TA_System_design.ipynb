{"cells":[{"cell_type":"markdown","metadata":{"id":"9yuL4idLUVk5"},"source":["# LLM as Teaching Assistants\n","\n","## Agenda\n","- Introduction to LLMs as Teaching Assistants\n","  - Responses in No Time\n","  - Cost Reduction\n","  - On-Demand Responses\n","- Understanding System Design of LLM as Teaching Assistants\n","- Discussing Key Concepts and Techniques\n","- Exploring Use Cases and Architecture\n","\n","---\n","\n","## What are LLMs?\n","\n","**Definition:** Large Language Models (LLMs) are advanced neural networks designed to process and understand human language. They are trained on vast datasets and are capable of generating contextually relevant human-like text.\n","\n","**Why train LLMs?**\n","- To generate human-like responses.\n","- To produce coherent text based on the context provided.\n","  \n","### Characteristics\n","- **Deep Neural Networks:** These models consist of numerous layers (hence 'deep') that enable them to learn complex patterns from large datasets.\n","- **Parameters:** The number of parameters ranges from millions to billions. More parameters generally lead to better model performance, but they also require more computing power.\n","\n","### Comparison with Small Language Models (SLMs)\n","- **Size and Scope:** LLMs can generate larger, more coherent text than SLMs, which might be limited in terms of complexity and length.\n","- **Use Cases:** LLMs are preferable for complex language tasks like translation, summarization, and question answering due to their sophisticated understanding.\n","\n","---\n","\n","## Basic Use Cases of LLMs\n","1. **Question Answering:** Providing instant responses to user queries.\n","2. **Translations:** Translating texts from one language to another accurately.\n","3. **Summarization:** Creating concise summaries of lengthy documents.\n","4. **Sentiment Analysis:** Analyzing texts for their emotional tone.\n","\n","---\n","\n","## What is the SwiGLU Activation Function?\n","\n","**Definition:** SwiGLU is a non-linear activation function that combines the advantages of both the Swish and GLU (Gated Linear Unit) activation functions. It helps the model learn complex patterns and dependencies more effectively than simpler activation functions like ReLU.\n","\n","### Formula:\n","$\\text{SwiGLU}(x) = \\text{Sigmoid}(x) \\cdot \\text{ReLU}(x)$\n","\n","\n","---\n","\n","## What are Rotary Embeddings?\n","\n","**Definition:** Rotary embeddings are a mechanism in neural networks to encode the positional information of words in a sequence, allowing the model to understand the order of words better than traditional absolute positional embeddings.\n","\n","### Importance\n","- **Context Awareness:** By using rotary embeddings, the model can better associate words with their respective contexts, which is crucial for understanding sentences correctly.\n","- **Robustness:** This method helps the model maintain meaningful relationships between words in complex sentences.\n","\n","---\n","\n","## Why LLAMA is Preferred over GPT?\n","\n","**LLAMA (Large Language Model Meta AI):**\n","- **Parameter Efficiency:** LLAMA models range from 7 billion to 65 billion parameters, making them efficient while maintaining performance.\n","- **Performance:** Studies show LLAMA can outperform larger models like GPT-3 in certain tasks, providing a higher quality responses with fewer resources.\n","- **Training Data:** LLAMA uses diverse datasets for training which enhances its understanding of various topics.\n","\n","---\n","\n","## Advantages of Using LLMs as Teaching Assistants\n","- **Cost Efficiency:** Reduces the need for hiring multiple human teaching assistants.\n","- **Instantaneous Responses:** Users receive answers without waiting time, which enhances learning.\n","- **Scalability:** Can handle multiple users simultaneously without degradation of service quality.\n","\n","---\n","\n","## Rough Sketch of LLM as Teaching Assistant Architecture\n","\n","```mermaid\n","flowchart TD\n","    User -->|Sends Query| A[Teaching Assistant System]\n","    A -->|Forwards Query| B[Chat GPT]\n","    B -->|Generates Response| A\n","    A -->|Responds to User| User\n","```\n","\n","### Functional Issues of This Architecture\n","1. **Quality of Responses:** If the user query is misunderstood, the model may provide irrelevant answers.\n","2. **Data Storage Costs:** Using external APIs may incur costs based on token usage.\n","\n","---\n","\n","## What is Retrieval Augmented Fine Tuning (RAFT)?\n","\n","**Definition:** RAFT is an advanced approach that combines the benefits of retrieval-augmented generation (RAG) with fine-tuning techniques, allowing models to leverage external knowledge efficiently during the response generation process.\n","\n","## Differences Between RAG and RAFT\n","- **RAG:** Focuses on retrieving relevant pieces of information from an external database to formulate responses.\n","- **RAFT:** Enhances RAG by incorporating fine-tuning on specific datasets post-retrieval, leading to more accurate contextually appropriate outputs.\n","\n","---\n","\n","## What is FAISS?\n","\n","**Definition:** FAISS (Facebook AI Similarity Search) is a library developed for efficient similarity search and clustering of dense vectors. It provides tools for high-performance nearest neighbor search.\n","\n","### Applications\n","- Used in LLMs for similarity search in RAG architectures. When a document is queried, FAISS helps identify closely related documents in terms of embeddings.\n","\n","## PostgreSQL Vector Database?\n","\n","PostgreSQL can be enhanced with extensions to support vector operations, allowing for efficient management and querying of embedding data.\n","\n","### Pgvector\n","- Pgvector is a PostgreSQL extension for vector similarity search. It enables storage of vectors and efficient querying based on similarity.\n","  \n","---\n","\n","## Detailed Architecture Analysis\n","\n","- **Video Upload:** The admin uploads videos to the server.\n","- **Transcription:** Videos are transcribed using services like Amazon Transcribe.\n","- **Storing in Vector DB:** Transcriptions are processed and stored in a vector DB (e.g., FAISS).\n","- **Query Handling:** User queries are directed to the vector DB to retrieve relevant information.\n","- **Chat GPT Integration:** Contextual prompts are passed to Chat GPT to generate answers based on retrieved data.\n","\n","```mermaid\n","flowchart TD\n","    A[Video Upload] --> B[Transcription]\n","    B --> C[Store in Vector DB]\n","    C --> D[User Query]\n","    D --> E[Retrieve Documents]\n","    E --> F[Send to Chat GPT]\n","    F --> G[Response to User]\n","```\n","\n","---\n","\n","## What is Amazon Transcribe?\n","\n","**Definition:** Amazon Transcribe is a fully managed automatic speech recognition (ASR) service that converts speech into text. This service is particularly useful for processing audio files into readable transcripts.\n","\n","### Differences Between PyPDF Loader and PyMuPDF Loader\n","\n","- **PyPDF Loader:** Specifically designed for reading PDF files and extracting text.\n","- **PyMuPDF Loader:** Offers broader functionalities, including working with text, images, and annotations in PDFs while being more flexible.\n","\n","---\n","\n","## AI Interviewer App Architecture\n","\n","**Architecture Overview:**\n","1. **User Speech to Text Conversion:** Using services like Whisper.\n","2. **Evaluation Logic:** Chat GPT evaluates responses based on predefined criteria.\n","3. **Text to Speech Feedback:** The application converts feedback text back into speech for user interaction.\n","\n","```mermaid\n","flowchart TD\n","    A[User Speech] --> B[Convert to Text]\n","    B --> C[Chat GPT Evaluation]\n","    C --> D[Text to Speech]\n","    D --> E[User Feedback]\n","```\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AIgZNRaRUVk7"},"source":["# LLM as Teaching Assistants - Extended Notes with Multimodal RAG\n","\n","## Multimodal Retrieval-Augmented Generation (RAG)\n","\n","### Introduction to Multimodal RAG\n","**Definition:** An advanced AI system that combines multiple data types (text, images, audio, video) to enhance language model capabilities by retrieving and processing information from multiple modalities simultaneously.\n","\n","**Why Needed?**\n","- Real-world scenarios often involve both visual and textual information (e.g., medical reports with X-rays and doctor's notes)\n","- Text and images provide complementary information (diagrams clarifying textual concepts)\n","- Standard text-only RAG misses visual context, leading to incomplete responses\n","\n","### Three Major Implementation Methods\n","\n","#### Option 1: Multimodal Embeddings + Multimodal LLM\n","1. **Embedding:** Use models like CLIP to encode images/text into shared vector space\n","2. **Retrieval:** Perform similarity search across both modalities\n","3. **Synthesis:** Pass raw images + text to multimodal LLM (e.g., GPT-4V)\n","\n","```mermaid\n","flowchart TD\n","    A[Text] --> B[Multimodal Embedding Model]\n","    C[Image] --> B\n","    B --> D[Vector Database]\n","    E[Query] --> F[Similarity Search]\n","    D --> F\n","    F --> G[Multimodal LLM]\n","    G --> H[Answer]\n","```\n","\n","#### Option 2: Image Summarization + Text Retrieval\n","1. **Summarization:** Use multimodal LLM to generate text descriptions of images\n","2. **Embedding:** Embed these summaries with standard text embeddings\n","3. **Synthesis:** Pass retrieved text to standard LLM\n","\n","#### Option 3: Hybrid Approach (Used in Project)\n","1. **Summarization:** Generate text summaries from images\n","2. **Embedding:** Store summaries with references to original images\n","3. **Synthesis:** Pass both raw images and text to multimodal LLM\n","\n","### Code Implementation Breakdown\n","\n","#### 1. PDF Processing with PyMuPDF\n","```python\n","import fitz  # PyMuPDF\n","import os\n","\n","def extract_images_from_pdf(pdf_path, output_folder):\n","    pdf_document = fitz.open(pdf_path)\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","    \n","    for page_number in range(len(pdf_document)):\n","        page = pdf_document.load_page(page_number)\n","        image_list = page.get_images(full=True)\n","        \n","        for img_index, img in enumerate(image_list):\n","            xref = img[0]\n","            base_image = pdf_document.extract_image(xref)\n","            image_bytes = base_image[\"image\"]\n","            image_ext = base_image[\"ext\"]\n","            image_filename = f\"page{page_number+1}_img{img_index+1}.{image_ext}\"\n","            image_filepath = os.path.join(output_folder, image_filename)\n","            \n","            with open(image_filepath, \"wb\") as image_file:\n","                image_file.write(image_bytes)\n","```\n","\n","Key Components:\n","- `get_images()`: Extracts all images from a PDF page\n","- `extract_image()`: Gets binary image data using cross-reference (xref)\n","- File naming convention preserves page/image relationships\n","\n","#### 2. Image Encoding and Description\n","```python\n","import base64\n","from openai import OpenAI\n","\n","def encode_image(image_path):\n","    with open(image_path, \"rb\") as image_file:\n","        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n","\n","def describe_image(base64_image):\n","    response = client.chat.completions.create(\n","        model=\"gpt-4-vision-preview\",\n","        messages=[\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"text\", \"text\": \"Extract ALL text from this image\"},\n","                    {\n","                        \"type\": \"image_url\",\n","                        \"image_url\": f\"data:image/png;base64,{base64_image}\"\n","                    }\n","                ]\n","            }\n","        ],\n","        max_tokens=300\n","    )\n","    return response.choices[0].message.content\n","```\n","\n","Key Points:\n","- Base64 encoding converts binary images to text-safe format\n","- GPT-4V processes both the image and textual prompt\n","- Response includes extracted text and image interpretation\n","\n","#### 3. Combined Text+Image Processing\n","```python\n","def extract_images_and_text_from_pdf(pdf_path, output_folder):\n","    pdf_document = fitz.open(pdf_path)\n","    combined_text = \"\"\n","    \n","    for page_number in range(len(pdf_document)):\n","        page = pdf_document.load_page(page_number)\n","        text = page.get_text()\n","        combined_text += f\"\\n\\nPage {page_number + 1}:\\n{text}\"\n","        \n","        image_list = page.get_images(full=True)\n","        for img_index, img in enumerate(image_list):\n","            # Image extraction code...\n","            base64_image = encode_image(image_filepath)\n","            image_description = describe_image(base64_image)\n","            combined_text += f\"\\n\\n[Image: {image_filename}]\\n{image_description}\"\n","    \n","    return combined_text\n","```\n","\n","Workflow:\n","1. Extracts raw text from each page\n","2. Processes each image through GPT-4V\n","3. Combines original text with image descriptions\n","4. Preserves references between text and images\n","\n","#### 4. Vector Database Setup\n","```python\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain_community.vectorstores import FAISS\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=500,\n","    chunk_overlap=60,\n","    separators=[\"\\n\\n\", \"\\n\"]\n",")\n","\n","splits = text_splitter.split_documents(loaders.load())\n","embeddings = OpenAIEmbeddings()\n","db = FAISS.from_documents(splits, embeddings)\n","```\n","\n","Key Configurations:\n","- Chunk size optimized for context preservation\n","- FAISS enables efficient similarity search\n","- OpenAI embeddings create semantic representations\n","\n","#### 5. Retrieval-Augmented QA System\n","```python\n","from langchain.chains import RetrievalQA\n","from langchain.prompts import PromptTemplate\n","\n","template = \"\"\"Use context to answer the question. Include image references when relevant:\n","{context}\n","Question: {question}\n","Answer:\"\"\"\n","QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n","\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n","    retriever=db.as_retriever(),\n","    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",")\n","\n","result = qa_chain({\"query\": \"Explain payment trends from the image\"})\n","```\n","\n","### Practical Applications\n","\n","1. **Education**\n","   - Lecture notes with diagrams (like the payment trends example)\n","   - Automated generation of alt-text for accessibility\n","\n","2. **E-commerce**\n","   - Product manuals with both textual instructions and diagrams\n","   - Visual search combined with textual queries\n","\n","3. **Medical**\n","   - Radiology reports combining scan images with doctor's notes\n","   - Prescription understanding with pill images\n","\n","### Challenges and Solutions\n","\n","| Challenge | Solution |\n","|-----------|----------|\n","| Large PDF processing | Chunking strategies + parallel processing |\n","| Image quality issues | Preprocessing with OpenCV/Pillow |\n","| Cost of multimodal LLMs | Hybrid approach (Option 3) |\n","| Reference preservation | Structured metadata in vector DB |\n","\n","### Advanced Concepts\n","\n","**CLIP Embeddings**\n","- Contrastive Language-Image Pretraining\n","- Enables cross-modal similarity search\n","- Can replace GPT-4V for retrieval when synthesis isn't needed\n","\n","**Pgvector Alternative**\n","```sql\n","CREATE TABLE document_embeddings (\n","    id SERIAL PRIMARY KEY,\n","    content TEXT,\n","    image_path VARCHAR(255),\n","    embedding VECTOR(1536)\n",");\n","\n","-- Find similar documents\n","SELECT content, image_path\n","FROM document_embeddings\n","ORDER BY embedding <=> '[query_embedding]'\n","LIMIT 5;\n","```\n","\n","### Future Directions\n","1. **Audio Integration** - Adding speech-to-text and audio analysis\n","2. **Video Processing** - Frame extraction + temporal understanding\n","3. **3D Model Support** - For engineering/CAD applications\n","\n","---\n","\n","## Integration with Teaching Assistant System\n","\n","```mermaid\n","flowchart TD\n","    A[Student Query] --> B{Contains Image?}\n","    B -->|Yes| C[Multimodal Processing]\n","    B -->|No| D[Standard RAG]\n","    C --> E[Image Extraction]\n","    E --> F[Description Generation]\n","    F --> G[Combined Embedding]\n","    D --> H[Text Embedding]\n","    G & H --> I[Vector DB Search]\n","    I --> J[Response Generation]\n","    J --> K[Student]\n","```\n","\n","**Benefits for Education:**\n","- Can explain textbook diagrams automatically\n","- Processes handwritten notes when shared as images\n","- Understands mathematical notation in scans\n","```"]},{"cell_type":"markdown","metadata":{"id":"NNQ4pUYDUVk8"},"source":["## Conclusion\n","\n","In this session, we explored the system design of LLMs as teaching assistants, delving into functionalities, architectures, and applications. Understanding how to utilize LLMs efficiently, especially in educational contexts, opens up possibilities for enhanced learning experiences.\n","\n","### Next Steps\n","- Review the LLM architecture and key concepts discussed.\n","- Implement a sample application based on provided architecture and code snippets.\n","- Explore additional literature on RAG and RAFT for a deeper understanding."]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}