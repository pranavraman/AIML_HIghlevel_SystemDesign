{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Real-Time Processing Notes\n",
    "\n",
    "## Overview\n",
    "- **Task**: Deploy Kafka on AWS EC2 for ML system.\n",
    "- **Kafka Features**:  \n",
    "  - Distributed, fault-tolerant, high-throughput.  \n",
    "  - Real-time data feeds (not a single message queue).  \n",
    "  - Durable storage with low latency.  \n",
    "  - Terms noted: \"Persistent Storage,\" \"Horizontal Scaling.\"\n",
    "\n",
    "---\n",
    "\n",
    "## System Components\n",
    "\n",
    "### Zookeeper\n",
    "- **Responsibilities**:  \n",
    "  - Status monitoring of services and data.\n",
    "  - Problem handling.\n",
    "  - Sending messages to topics.\n",
    "  - Partition key distribution (text-based).\n",
    "\n",
    "### Kafka Cluster Setup\n",
    "- **Brokers**:  \n",
    "  - **Broker 1**: \n",
    "    - Partition 0 (Leader), Partition 1 (Follower).\n",
    "  - **Broker 2**: \n",
    "    - Partitions 0 (Leader), 1 (Follower).\n",
    "- **Consumer**: Listens and reads data from specified topics.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-Time Stock Market Analysis Design\n",
    "\n",
    "### Functional Requirements\n",
    "1. **Real-time data ingestion**: \n",
    "   - Ingest prices, volumes, metadata every 1 second.\n",
    "2. **Stream processing**: \n",
    "   - Data cleaning, aggregation, and transformation in real-time.\n",
    "3. **Batch and real-time storage**: \n",
    "   - Store raw and processed data in a query-optimized format (e.g., S3).\n",
    "4. **Real-time alerts**: \n",
    "   - Detect anomalies (volume spikes, price drops).\n",
    "5. **Batch analytics**: \n",
    "   - Execute SQL queries on historical data.\n",
    "6. **ML predictions**: \n",
    "   - Predict short-term stock trends.\n",
    "7. **Dashboards**: \n",
    "   - Visualize trends.\n",
    "\n",
    "### Non-Functional Requirements\n",
    "- **Low latency**: \n",
    "  - Aim for < 5 seconds (end-to-end data insight).\n",
    "- **Scalability**: \n",
    "  - Support 1000 stocks with 1-second updates.\n",
    "- **Fault tolerance**: \n",
    "  - Systems should remain operational in case of failure.\n",
    "- **Cost efficiency**: \n",
    "  - Use S3 storage and EC2 virtual machines.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Processes\n",
    "1. **Real-Time Anomaly Detection**:\n",
    "   - Identify volume spikes and price drops.\n",
    "2. **Batch Analysis**:\n",
    "   - Conduct SQL queries on historical data.\n",
    "3. **ML Predictions**:\n",
    "   - Short-term stock trend forecasting.\n",
    "4. **Visualization**:\n",
    "   - Display trends using dashboards.\n",
    "\n",
    "### Data Sources\n",
    "- **APIs**: \n",
    "  - Yahoo Finance, Alpha Vantage API.\n",
    "\n",
    "---\n",
    "\n",
    "## Kafka on EC2 Deployment Steps\n",
    "1. **EC2 Setup**:  \n",
    "   - Launch an Amazon Linux 2 instance.  \n",
    "   - Connect via SSH using a key-value pair.  \n",
    "2. **Install Dependencies**:  \n",
    "   ```bash\n",
    "   sudo yum install java\n",
    "   wget https://archive.apache.org/dist/kafka/3.3.1/kafka_2.12-3.3.1.tgz\n",
    "   tar -xzf kafka_2.12-3.3.1.tgz\n",
    "   cd kafka_2.12-3.3.1/\n",
    "   ```\n",
    "3. **Start Services**:  \n",
    "   - **Start Zookeeper (Background)**:  \n",
    "     ```bash\n",
    "     bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "     ```\n",
    "   - **Start Kafka server (New Terminal)**:  \n",
    "     ```bash\n",
    "     export KAFKA_HEAP_OPTS=\"-Xmx256M -Xms128M\"\n",
    "     bin/kafka-server-start.sh config/server.properties\n",
    "     ```\n",
    "4. **S3 Integration**:  \n",
    "   - Raw data path: `s3://stock-data/raw/data=2023-10-10/hours=12/example.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "## ETL & Analytics\n",
    "\n",
    "### AWS Services\n",
    "- **Glue Jobs**: \n",
    "  - Convert JSON to structured data.\n",
    "  - Clean and process the data (e.g., handle nulls, calculate moving averages).\n",
    "- **Processed Data Path**:  \n",
    "  `s3://stock-data/processed/date/hour/`.\n",
    "\n",
    "### AWS Athena: SQL Example\n",
    "```sql\n",
    "CREATE EXTERNAL TABLE my_table (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    age INT\n",
    ")\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 's3://my-bucket/data/';\n",
    "```\n",
    "\n",
    "### AWS Lambda (Optional)\n",
    "```python\n",
    "def lambda_handler(event, context):\n",
    "    for record in event['Records']:\n",
    "        data = json.loads(record['value'])\n",
    "        if data['volume'] > 1000000:\n",
    "            sns.publish(TopicArn='arn:aws:sns:...', Message=f\"Volume spike: {data['symbol']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Modeling & Predictions\n",
    "\n",
    "### Feature Engineering\n",
    "- **Moving Average**:  \n",
    "```python\n",
    "df['5min_MA'] = df['close'].rolling(window=300).mean()  # Rolling average for 5 minutes\n",
    "```\n",
    "\n",
    "### Models\n",
    "- **XGBoost**:\n",
    "  - Pros: Fast, good for structured/tabular data.\n",
    "  - Best use: Short-term predictions.\n",
    "\n",
    "- **LSTM**: \n",
    "  - Captures temporal patterns, handles volatility well.\n",
    "  - Requires larger datasets; suited for high-frequency trading.\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(60, 5)),  # 60 time steps, 5 features\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "```\n",
    "\n",
    "### Deployment\n",
    "- Predictions sent to dashboards/APIs.\n",
    "- Integration possibilities: AWS SageMaker, AWS Lambda.\n",
    "\n",
    "---\n",
    "\n",
    "## Production Considerations\n",
    "- **LSTM latency**: Processing times may impose constraints.\n",
    "- **Kafka Integration**: Ensure seamless connection between services.\n",
    "- **Zero Spend Budget**: Set alerts on expenses in AWS to prevent unexpected charges.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Diagram using Mermaid\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Stock Data Source - Yahoo Finance, Alpha Vantage] --> B[Kafka on EC2]\n",
    "    B --> C[Real Time Alerting - AWS Lambda]\n",
    "    B --> D[Store Raw Data in S3]\n",
    "    C --> D\n",
    "    D --> E[Transformation using Glue]\n",
    "    E --> F[Store Processed Data in S3]\n",
    "    F --> G[AWS Athena SQL Queries]\n",
    "    F --> H[Model Predictions - XGBoost, LSTM]\n",
    "    H --> I[Deploy Model to SageMaker]\n",
    "    I --> J[Dashboard Visualization]\n",
    "```\n",
    "\n",
    "# Continuing from Kafka Real-Time Processing Notes\n",
    "\n",
    "## Billing in AWS\n",
    "### AWS Billing Overview\n",
    "- **Understanding Charges and Costs**:\n",
    "  - AWS operates on a pay-as-you-go model, meaning you pay for what you use, which can include services like EC2, S3, Lambda, and others.\n",
    "  \n",
    "### Zero Spend Budget\n",
    "- **Purpose of Zero Spend Budget**:\n",
    "  - This feature allows you to set a budget threshold (e.g., $0.01), which sends an alert when your spending reaches that limit.\n",
    "  - It is crucial for new users to prevent unexpected charges while experimenting and learning about AWS services.\n",
    "\n",
    "### Steps to Set Up a Zero Spend Budget:\n",
    "1. **Navigate to Billings Dashboard**:\n",
    "   - Go to your AWS Management Console and find the “Billing and Cost Management” section.\n",
    "   \n",
    "2. **Create a Budget**:\n",
    "   - Click on “Budgets” and then “Create Budget.”\n",
    "   - Select “Cost Budget” and choose “Use a template.”\n",
    "\n",
    "3. **Set Budget Type**:\n",
    "   - Choose “Zero Spend Budget” to avoid any charges that exceed the threshold.\n",
    "   - Set budget conditions based on anticipated usage of different services.\n",
    "\n",
    "4. **Email Notifications**:\n",
    "   - Add your email address to receive alerts when your spending approaches or exceeds the defined limit.\n",
    "   \n",
    "5. **Review and Confirm**:\n",
    "   - Review your settings to ensure accuracy and confirm to create the budget.\n",
    "\n",
    "---\n",
    "\n",
    "## External Tables in AWS Athena\n",
    "### What is an External Table?\n",
    "- **Definition**: \n",
    "  - An external table in AWS Athena allows you to query data stored in S3 without the need for loading it into a database.\n",
    "  - It provides a way to run SQL queries directly from the data stored in S3, treating it as a performable dataset.\n",
    "\n",
    "### Steps to Create an External Table:\n",
    "1. **Use the AWS Athena Console**:\n",
    "    - Navigate to the Athena service in the AWS Management Console.\n",
    "\n",
    "2. **Select Database**:\n",
    "   - Choose the database where you want to create the external table.\n",
    "\n",
    "3. **Run SQL Command**:\n",
    "   - Use the following SQL syntax to create your external table:\n",
    "   ```sql\n",
    "   CREATE EXTERNAL TABLE my_table (\n",
    "       id INT,\n",
    "       name STRING,\n",
    "       age INT\n",
    "   )\n",
    "   ROW FORMAT DELIMITED \n",
    "   FIELDS TERMINATED BY ','\n",
    "   STORED AS TEXTFILE\n",
    "   LOCATION 's3://my-bucket/data/';\n",
    "   ```\n",
    "   - Make sure to replace `'s3://my-bucket/data/'` with the actual S3 path where your data files are stored.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "### Review of Key Concepts\n",
    "1. **Kafka as a Streaming Platform**:\n",
    "   - Understand Kafka's role in handling high-throughput, fault-tolerant messaging for real-time updates, such as stock prices.\n",
    "\n",
    "2. **Setting Up in AWS**:\n",
    "   - Knowledge of launching EC2 instances, setting up Kafka and Zookeeper, and managing lifecycle (start/stop/terminate).\n",
    "\n",
    "3. **Real-Time Processing and Analysis**:\n",
    "   - Combine tools like AWS Glue for ETL processes, AWS Athena for querying, and integration of alerts through AWS Lambda.\n",
    "\n",
    "4. **Cost Management**:\n",
    "   - Setting up a Zero Spend Budget is essential for managing costs effectively in AWS, especially for beginners.\n",
    "\n",
    "5. **Database Interactions**:\n",
    "   - Utilize external tables effectively in Athena, allowing flexible analysis of data located in S3.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "- **Practice Deployment**: \n",
    "  - Ensure that all utility components (Kafka, Zookeeper, EC2 instances) are practiced.\n",
    "  - Explore real-time data ingestion through Kafka by simulating stock price updates.\n",
    "\n",
    "- **Engage with AWS Services**: \n",
    "  - Familiarize yourself with other relevant services such as AWS Glue for ETL, AWS Lambda for serverless processing, and AWS SageMaker for model deployment.\n",
    "\n",
    "- **Discussion and Troubleshooting**: \n",
    "  - Be prepared to share insights or challenges faced during implementations in the next class session so solutions can be collaboratively identified."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
