{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5973ae7d",
   "metadata": {},
   "source": [
    "# Shazam Algorithm and System Design Notes for ML System Design Classes\n",
    "\n",
    "### Key Points\n",
    "- Shazam identifies songs by creating unique audio fingerprints from short audio samples, likely using signal processing techniques rather than heavy machine learning.\n",
    "- The process involves recording audio, applying Fourier transforms to create spectrograms, generating hashes from significant peaks, and matching them against a database.\n",
    "- The system is designed to be robust to noise, scalable for millions of users, and efficient in delivering fast results.\n",
    "- While primarily used for music recognition, the technology can be applied to voice identification, plagiarism detection, and other audio-based applications.\n",
    "- The evidence leans toward Shazam relying on traditional algorithms like Fourier transforms and hashing, though modern enhancements might incorporate machine learning for specific tasks.\n",
    "\n",
    "### Overview of Shazam\n",
    "Shazam is a mobile application that identifies music, movies, and other media by analyzing short audio clips (typically 5-10 seconds) captured via a device's microphone. Launched in 2002, it uses audio fingerprinting to match user-recorded audio against a database of known songs, providing details like song title, artist, and lyrics. This process is a classic example of system design in machine learning, balancing accuracy, scalability, and speed.\n",
    "\n",
    "### How It Works\n",
    "When you record a song snippet with Shazam, the app processes the audio to create a unique fingerprint based on its frequency patterns. This fingerprint is compared to a vast database to find a match, even if the audio is noisy or recorded in a busy environment. The system then retrieves and displays metadata about the matched song, such as the artist and album.\n",
    "\n",
    "### Why It’s Effective\n",
    "Shazam’s algorithm is designed to handle real-world challenges like background noise or partial audio. It uses mathematical techniques to ensure fast and accurate matching, making it reliable for millions of users worldwide. Its scalability and efficiency make it a great case study for system design.\n",
    "\n",
    "### Applications Beyond Music\n",
    "The technology behind Shazam can be used for tasks like identifying speakers in meetings, detecting copyrighted content in videos, or recognizing specific sounds in security systems, showcasing its versatility in audio processing.\n",
    "\n",
    "---\n",
    "\n",
    "# Detailed Notes for ML System Design Classes: Shazam Algorithm and System Design\n",
    "\n",
    "These notes provide a comprehensive overview of Shazam’s algorithm and system design, tailored for ML System Design classes. They are structured for clarity, using Markdown, and include detailed explanations, examples, code snippets, and flowcharts to aid understanding. The content is based on a lecture transcription and supplemented with insights from reliable sources, ensuring technical accuracy while being accessible to freshers.\n",
    "\n",
    "## 1. Introduction to Shazam\n",
    "\n",
    "### What is Shazam?\n",
    "Shazam is a mobile application that identifies music, movies, advertising, and television shows by analyzing short audio samples (5-10 seconds) captured via a device’s microphone ([Toptal: Shazam Algorithm](https://www.toptal.com/algorithms/shazam-it-music-processing-fingerprinting-and-recognition)). Launched in 2002, it has become a leading tool for music discovery, allowing users to identify songs playing in their environment and access metadata like artist, album, and lyrics.\n",
    "\n",
    "### Relevance to ML System Design\n",
    "Shazam’s functionality relies on audio fingerprinting, signal processing, and scalable database design, making it a prime example for studying ML system design. It demonstrates how to handle large-scale data, ensure low-latency responses, and maintain robustness against noise, all critical aspects of designing real-world ML systems.\n",
    "\n",
    "## 2. Historical Context\n",
    "\n",
    "### Shazam’s Evolution in 2003\n",
    "Shazam was launched in 2002, and its algorithm was detailed in 2003 by inventor Avery Li-Chung Wang ([Columbia: Shazam Paper](http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf)). In 2003, machine learning was not as prevalent as today, so Shazam relied on signal processing techniques, particularly the Fast Fourier Transform (FFT), to create audio fingerprints. This approach was computationally efficient and effective for the technology available at the time.\n",
    "\n",
    "## 3. Core Concept: Audio Fingerprinting\n",
    "\n",
    "### Definition and Importance\n",
    "Audio fingerprinting involves creating a compact, unique representation of an audio signal that can be matched against a database to identify the source ([ACRCloud: Shazam Technology](https://blog.acrcloud.com/how-does-shazam-work)). Similar to human fingerprints, audio fingerprints capture distinctive features of a song, enabling fast and accurate identification even with short or noisy samples. In ML system design, fingerprinting is a key technique for efficient pattern matching.\n",
    "\n",
    "### Example\n",
    "Imagine hearing a song in a noisy café. Shazam records a 10-second clip, extracts its unique frequency patterns, and matches them to a database, identifying the song despite background chatter.\n",
    "\n",
    "## 4. Functional Requirements\n",
    "\n",
    "### System Capabilities\n",
    "Shazam must support the following:\n",
    "- **Audio Input:** Capture 5-10 seconds of audio via the device’s microphone.\n",
    "- **Song Identification:** Accurately identify the song from the audio snippet.\n",
    "- **Metadata Display:** Provide details like song title, artist, album, and lyrics.\n",
    "- **No Match Handling:** Inform users if no match is found.\n",
    "- **User History:** Store a history of identified songs for user reference.\n",
    "\n",
    "## 5. Non-Functional Requirements\n",
    "\n",
    "### Performance Goals\n",
    "To ensure a robust user experience, Shazam must meet these non-functional requirements:\n",
    "- **Accuracy:** High accuracy despite noise or partial audio.\n",
    "- **Latency:** Response times within seconds.\n",
    "- **Scalability:** Handle millions of simultaneous queries.\n",
    "- **Availability:** Maintain high uptime for reliability.\n",
    "- **Efficiency:** Optimize computational and storage resources.\n",
    "- **Updateability:** Regularly update the database with new songs.\n",
    "\n",
    "## 6. Data Types\n",
    "\n",
    "### Types of Data Handled\n",
    "Shazam processes three main data types:\n",
    "- **Audio Data:** Raw user audio (noisy, varying loudness) and reference songs (clean, high-quality).\n",
    "- **Fingerprint Data:** Hashes and timestamps representing unique audio patterns.\n",
    "- **Metadata:** Song details (artist, album, lyrics) stored in databases.\n",
    "\n",
    "### Data Characteristics\n",
    "User audio may include background noise, varying volumes, or distortions, requiring robust processing. Reference songs are typically high-quality digital files.\n",
    "\n",
    "## 7. Algorithmic Process\n",
    "\n",
    "### How Shazam Identifies Songs\n",
    "Shazam’s algorithm involves six key steps, detailed below with technical insights and code examples.\n",
    "\n",
    "#### Step 1: Record Raw Audio\n",
    "- **Description:** Capture 5-10 seconds of audio at 44,100 Hz, based on the Nyquist-Shannon Theorem, to cover frequencies up to 20,000 Hz ([Toptal: Shazam Algorithm](https://www.toptal.com/algorithms/shazam-it-music-processing-fingerprinting-and-recognition)).\n",
    "- **Example Code:**\n",
    "```python\n",
    "import pyaudio\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "audio = pyaudio.PyAudio()\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "frames = []\n",
    "for i in range(0, int(RATE / CHUNK * 10)):  # Record for 10 seconds\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "```\n",
    "\n",
    "#### Step 2: Clean Audio\n",
    "- **Description:** Apply minimal noise reduction and normalization to handle varying audio quality. Shazam’s algorithm is designed to be robust to noise, reducing the need for extensive cleaning.\n",
    "- **Example Code:**\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "fs, audio_data = wavfile.read('audio.wav')\n",
    "audio_data = audio_data / np.max(np.abs(audio_data))  # Normalize to [-1, 1]\n",
    "```\n",
    "\n",
    "#### Step 3: Fourier Transform\n",
    "- **Description:** Use the Fast Fourier Transform (FFT) to convert the time-domain audio signal into a frequency-domain spectrogram, showing frequency vs. time ([Steemit: Music Recognition](https://steemit.com/technology/%40phenom/how-does-shazam-work-let-s-understand-music-recognition-algorithms-together)).\n",
    "- **Example Code:**\n",
    "```python\n",
    "from scipy.signal import spectrogram\n",
    "f, t, Sxx = spectrogram(audio_data, fs=44100)\n",
    "```\n",
    "\n",
    "#### Step 4: Create Fingerprints\n",
    "- **Description:** Identify peaks in the spectrogram (landmarks) in specific frequency intervals (e.g., 30-40 Hz, 40-80 Hz), compute relative differences, and generate hashes with a fuzz factor for noise tolerance ([TechAhead: Shazam Recognition](https://www.techaheadcorp.com/blog/decoding-shazam-how-does-music-recognition-work-with-shazam-app/)).\n",
    "- **Example Code:**\n",
    "```python\n",
    "from scipy.signal import find_peaks\n",
    "peaks, _ = find_peaks(Sxx, height=0.1)  # Adjust height as needed\n",
    "```\n",
    "\n",
    "#### Step 5: Build Database\n",
    "- **Description:** Store hashes with timestamps and song IDs in a scalable NoSQL database like Cassandra or HBase for fast lookups.\n",
    "- **Example Code:**\n",
    "```python\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('fingerprints.db')\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS fingerprints\n",
    "             (hash TEXT, time REAL, song_id TEXT)''')\n",
    "c.execute(\"INSERT INTO fingerprints VALUES (?, ?, ?)\", (hash_value, time, song_id))\n",
    "conn.commit()\n",
    "```\n",
    "\n",
    "#### Step 6: Matching Process\n",
    "- **Description:** Generate hashes from query audio, search the database for matches, and verify timing consistency to identify the song ([MakeUseOf: Shazam Accuracy](https://www.makeuseof.com/how-does-shazam-work/)).\n",
    "- **Example Code:**\n",
    "```python\n",
    "query_hashes = compute_hashes(query_audio)\n",
    "matches = []\n",
    "for h in query_hashes:\n",
    "    result = c.execute(\"SELECT song_id, time FROM fingerprints WHERE hash = ?\", (h,))\n",
    "    matches.extend(result.fetchall())\n",
    "# Group by song_id and check timing consistency\n",
    "```\n",
    "\n",
    "## 8. System Design Components\n",
    "\n",
    "### Architecture Overview\n",
    "Shazam’s system is designed for scalability and low latency, with the following components:\n",
    "- **API Gateway:** Manages incoming requests from mobile apps.\n",
    "- **Load Balancers:** Distribute requests across servers.\n",
    "- **Fingerprint Conversion Servers:** Process audio to fingerprints.\n",
    "- **Matching Servers:** Compare fingerprints to the database.\n",
    "- **Databases:**\n",
    "  - **Fingerprint Database:** NoSQL (e.g., Cassandra, HBase) for fast hash lookups.\n",
    "  - **Metadata Database:** Relational (PostgreSQL) or NoSQL (MongoDB) for song details.\n",
    "- **Response Routing:** Delivers results to the user’s app.\n",
    "\n",
    "### Database Choices\n",
    "| Database Type | Example | Use Case | Pros | Cons |\n",
    "|---------------|---------|----------|------|------|\n",
    "| NoSQL | Cassandra, HBase | Fingerprint storage | High scalability, fast lookups | Limited query flexibility |\n",
    "| Relational | PostgreSQL | Metadata storage | Complex queries, structured data | Less scalable for massive datasets |\n",
    "| NoSQL | MongoDB | Metadata storage | Flexible schemas, unstructured data | May require joins for relationships |\n",
    "\n",
    "## 9. Robustness to Noise\n",
    "\n",
    "### Handling Noisy Audio\n",
    "Shazam’s algorithm is robust to noise due to:\n",
    "- **Relative Differences:** Using pairs of peaks (frequency and time differences) rather than absolute values.\n",
    "- **Fuzz Factor:** Quantizing hashes to tolerate minor distortions.\n",
    "- **Example:** In a noisy environment, a peak at 440 Hz might shift slightly, but the relative difference to another peak (e.g., 880 Hz) remains consistent, allowing accurate matching.\n",
    "\n",
    "## 10. Scalability and Performance\n",
    "\n",
    "### Scaling Strategies\n",
    "- **Distributed Systems:** Fingerprint database is sharded across multiple servers.\n",
    "- **Parallel Processing:** Queries are processed in parallel for speed.\n",
    "- **Caching:** Popular songs’ fingerprints are cached to reduce database load.\n",
    "- **CDNs:** Content Delivery Networks distribute metadata efficiently.\n",
    "- **Performance:** Shazam handles millions of queries daily with sub-second response times.\n",
    "\n",
    "## 11. Use Cases Beyond Shazam\n",
    "\n",
    "### Additional Applications\n",
    "Shazam’s fingerprinting technology can be applied to:\n",
    "- **Voice Identification:** Identifying speakers in meetings (e.g., Google Meet note-taker).\n",
    "- **Plagiarism Detection:** Detecting copyrighted music in videos (e.g., YouTube).\n",
    "- **Environmental Monitoring:** Recognizing specific sounds like animal calls.\n",
    "- **Security Systems:** Detecting alarms or other critical sounds.\n",
    "\n",
    "### Example\n",
    "In a Google Meet scenario, audio fingerprinting could identify speakers by creating unique voice fingerprints, similar to song fingerprints, enabling automated note-taking by associating speech with individuals.\n",
    "\n",
    "## 12. Comparison of Techniques\n",
    "\n",
    "### Traditional vs. ML Approaches\n",
    "- **Traditional (Shazam):** Relies on FFT and hashing, which are computationally efficient and scalable for song identification.\n",
    "- **ML/Deep Learning:** Could enhance accuracy for complex scenarios (e.g., distinguishing similar songs) but is often overkill due to increased computational demands.\n",
    "- **Trade-offs:** Traditional methods are faster and require less training data, while ML offers flexibility for nuanced tasks but at higher cost.\n",
    "\n",
    "## 13. Flowchart of Shazam’s Process\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[User Records Audio] --> B[Clean Audio]\n",
    "    B --> C[Fourier Transform]\n",
    "    C --> D[Create Spectrogram]\n",
    "    D --> E[Identify Landmarks]\n",
    "    E --> F[Compute Hashes]\n",
    "    F --> G[Store Fingerprints in Database]\n",
    "    H[Query Audio] --> I[Clean Audio]\n",
    "    I --> J[Fourier Transform]\n",
    "    J --> K[Create Spectrogram]\n",
    "    K --> L[Identify Landmarks]\n",
    "    L --> M[Compute Hashes]\n",
    "    M --> N[Search Database]\n",
    "    N --> O[Find Matches]\n",
    "    O --> P[Retrieve Metadata]\n",
    "    P --> Q[Display Results]\n",
    "```\n",
    "\n",
    "## 14. Summary of Key Points\n",
    "- **Core Idea:** Audio fingerprinting enables fast, accurate music recognition.\n",
    "- **Algorithm:** Uses FFT, spectrograms, and hashing to create and match fingerprints.\n",
    "- **System Design:** Scalable databases and distributed servers ensure performance.\n",
    "- **Robustness:** Handles noise through relative differences and fuzz factors.\n",
    "- **Applications:** Extends to voice identification, plagiarism detection, and more.\n",
    "\n",
    "## Key Citations\n",
    "- [How does Shazam work? Music Recognition Algorithms, Fingerprinting, and Processing](https://www.toptal.com/algorithms/shazam-it-music-processing-fingerprinting-and-recognition)\n",
    "- [How Does Shazam Work? Let's Understand Music Recognition Algorithms Together](https://steemit.com/technology/%40phenom/how-does-shazam-work-let-s-understand-music-recognition-algorithms-together)\n",
    "- [abracadabra: How does Shazam work?](https://www.cameronmacleod.com/blog/how-does-shazam-work)\n",
    "- [How does the Shazam's algorithm work?](https://www.quora.com/How-does-the-Shazams-algorithm-work)\n",
    "- [How does Shazam work](http://coding-geek.com/how-shazam-works/)\n",
    "- [How does the Shazam app recognize music?](https://www.techaheadcorp.com/blog/decoding-shazam-how-does-music-recognition-work-with-shazam-app/)\n",
    "- [How does Shazam work?](https://blog.acrcloud.com/how-does-shazam-work)\n",
    "- [How does Shazam work? What is the logic behind Shazam tracing out the exact song by just a sample of it?](https://www.quora.com/How-does-Shazam-work-What-is-the-logic-behind-Shazam-tracing-out-the-exact-song-by-just-a-sample-of-it)\n",
    "- [An Industrial-Strength Audio Search Algorithm](http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf)\n",
    "- [How Does Shazam Recognize Music Accurately?](https://www.makeuseof.com/how-does-shazam-work/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
